{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALjerUyZFOya",
        "outputId": "27cb168c-87e0-4752-9f27-0c1e67a3d522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEl8Kpv-FVyo"
      },
      "outputs": [],
      "source": [
        "#create dataloaders for training\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define paths to stored pickle files\n",
        "DATASET_PATH = \"/content/drive/My Drive/Colab Notebooks/APS360-project/Chord_Pandas_Dataset_specto\"\n",
        "train_pkl = os.path.join(DATASET_PATH, \"train.pkl\")\n",
        "val_pkl = os.path.join(DATASET_PATH, \"val.pkl\")\n",
        "test_pkl = os.path.join(DATASET_PATH, \"test.pkl\")\n",
        "\n",
        "# Load datasets from pickle files\n",
        "df_train = pd.read_pickle(train_pkl)\n",
        "df_val = pd.read_pickle(val_pkl)\n",
        "df_test = pd.read_pickle(test_pkl)\n",
        "\n",
        "# Encode labels into integers\n",
        "label_encoder = LabelEncoder()\n",
        "df_train[\"label\"] = label_encoder.fit_transform(df_train[\"label\"])\n",
        "df_val[\"label\"] = label_encoder.transform(df_val[\"label\"])\n",
        "df_test[\"label\"] = label_encoder.transform(df_test[\"label\"])\n",
        "\n",
        "class ChromaDatasetResNet(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #expect a (12, T) shape\n",
        "        chroma = self.df.iloc[idx][\"pcp\"][0]\n",
        "        label = self.df.iloc[idx][\"label\"]\n",
        "\n",
        "        #convert chromagram to image\n",
        "        #scale to [0, 255] and convert to unit8\n",
        "        img = (np.clip(chroma, 0, 1) * 255).astype(np.uint8)\n",
        "        #transpose so time is the horiztontal component\n",
        "        chroma_img = Image.fromarray(img.T)\n",
        "        chroma_img = chroma_img.convert(\"RGB\") #grayscale -> colour\n",
        "\n",
        "        if self.transform:\n",
        "            chroma_img = self.transform(chroma_img)\n",
        "\n",
        "        return chroma_img, label\n",
        "\n",
        "# Create ImageNet style images by defining transforms\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Create datasets and Dataloaders\n",
        "train_dataset_resnet = ChromaDatasetResNet(df_train, transform=data_transforms['train'])\n",
        "val_dataset_resnet   = ChromaDatasetResNet(df_val, transform=data_transforms['val'])\n",
        "test_dataset_resnet  = ChromaDatasetResNet(df_test, transform=data_transforms['val'])\n",
        "\n",
        "train_loader_resnet = DataLoader(train_dataset_resnet, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader_resnet   = DataLoader(val_dataset_resnet, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader_resnet  = DataLoader(test_dataset_resnet, batch_size=32, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7svRpWUWNwb1"
      },
      "outputs": [],
      "source": [
        "classes_num = len(train_dataset_resnet.df[\"label\"].unique())\n",
        "\n",
        "#load pretrained ResNet model\n",
        "resnet_model = models.resnet18(pretrained=True)\n",
        "\n",
        "#freeze fall layers first and then unfreeze layer 4 and fc\n",
        "for name, param in resnet_model.named_parameters():\n",
        "    #freeze all\n",
        "    param.requires_grad = False\n",
        "    if \"layer4\" in name or \"fc\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "#replace the last FC layer to output the chord classes\n",
        "in_features = resnet_model.fc.in_features\n",
        "resnet_model.fc = nn.Linear(in_features, classes_num)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet_model = resnet_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOLoAU2IOqyQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train_model_resnet(model, train_loader, val_loader, num_epochs, criterion, optimizer):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "  start_time = time.time()\n",
        "  train_acc_history = np.zeros(num_epochs)\n",
        "  val_acc_history = np.zeros(num_epochs)\n",
        "  train_loss_history = np.zeros(num_epochs)\n",
        "  val_loss_history = np.zeros(num_epochs)\n",
        "\n",
        "  print(\"Starting Training\")\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "\n",
        "    for features, labels in train_loader:\n",
        "      features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(features)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      train_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    train_loss_history[epoch] = train_loss/ len(train_loader)\n",
        "    train_acc_history[epoch] = train_correct / len(train_loader.dataset)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for features, labels in val_loader:\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        val_loss += loss.item()\n",
        "        val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    val_loss_history[epoch] = val_loss / len(val_loader)\n",
        "    val_acc_history[epoch] = val_correct / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss_history[epoch]:.4f}, Train Acc: {train_acc_history[epoch]:.4f}, || Val Loss: {val_loss_history[epoch]:.4f}, Val Acc: {val_acc_history[epoch]:.4f}\")\n",
        "\n",
        "  print(\"Finished Training\")\n",
        "  end_time = time.time()\n",
        "  print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "  #save training files\n",
        "  path = \"/content/drive/My Drive/Colab Notebooks/APS360-project/resnet\"\n",
        "  np.savetxt(\"{}train_acc.csv\".format(path), train_acc_history)\n",
        "  np.savetxt(\"{}val_acc.csv\".format(path), val_acc_history)\n",
        "  np.savetxt(\"{}train_loss.csv\".format(path), train_loss_history)\n",
        "  np.savetxt(\"{}val_loss.csv\".format(path), val_loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "ijiNfC6XQRVH",
        "outputId": "5a3ce1a2-5867-47bc-fd65-40887533a2f0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f45da6775b60>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m optimizer = optim.Adam([\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresnet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#lower learning rate for tuning layer 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresnet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m#higher learning rate for final layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam([\n",
        "    {\"params\": resnet_model.layer4.parameters(), \"lr\": 0.0001}, #lower learning rate for tuning layer 4\n",
        "    {\"params\": resnet_model.fc.parameters(), \"lr\": 0.001} #higher learning rate for final layer\n",
        "])\n",
        "\n",
        "train_model_resnet(resnet_model, train_loader_resnet, val_loader_resnet, 40, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07-5l7G6QqWW"
      },
      "outputs": [],
      "source": [
        "#test model function\n",
        "def test_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            test_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    avg_loss = test_loss / len(test_loader)\n",
        "    accuracy = test_correct / len(test_loader.dataset)\n",
        "\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvBO8uoTQy6B"
      },
      "outputs": [],
      "source": [
        "#testing accuracy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "test_model(resnet_model, test_loader_resnet, criterion)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}